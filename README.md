# Hardware_Progression
# ðŸ–¥ï¸ Virtualization Journey: From Laptop Bottlenecks to Hardened Lab

This document tracks the evolution of my home lab â€” from early experiments on a laptop to enterpriseâ€‘class workstations, networking, and backup strategies. Each phase highlights bottlenecks, upgrades, and lessons learned.

---

## Phase 1: Early Experiments on Limited Hardware
- **Starting Point:** Acer laptop, 2 cores, 8â€¯GB RAM  
- **Challenge:** Running 2 VMs in VirtualBox was painfully slow  
- **Diagnosis:**  
  - CPU and RAM underutilized  
  - Disk pegged at 100% usage  
  - Mechanical HDD throughput only 8â€“20â€¯MB/s  
- **Upgrades:**  
  - Crucial 1â€¯TB SSD (~360â€¯MB/s)  
  - RAM increased to 16â€¯GB (partial disassembly required)  
- **Result:** Major uplift in VM responsiveness  
- **Extra:** Replaced failing keyboard  

---

## Phase 2: Professional Exposure (Logitrain Internship)
- **Environment:** VMware ESXi booted from USB, dedicated VM storage drive  
- **Observed Bottleneck:** Disk throughput limited (20â€“45â€¯MB/s)  
- **Lesson:** Storage throughput dominates virtualization performance  

---

## Phase 3: Highâ€‘Performance Storage
- **Procurement:** M.2 NVMe drives + ASUS Hyper card (PCIe Gen4, bifurcation support)  
- **Performance Gains:**  
  - 3,500â€¯MB/s per drive  
  - Up to 4,800â€¯MB/s in parallel  
- **Architecture:**  
  - NVMe for active workloads  
  - 2â€¯TB HDD for cold storage/backups  
- **RAID Consideration:** Rejected RAID on NVMe (wear/failure risk, RAID â‰  backup)  

---

## Phase 4: Backup & Recovery Strategy
- **Spare PC:** Runs Veeam for backups  
- **Coverage:** Entire laptop + C: and D: drives on main PC  
- **Gap:** NVMe workloads not backed up due to size  
- **Discovery:** Some VMs not easily replaceable (e.g., Kali Gen2 requirement)  
- **Permissions Issue:** VM files locked to single PC â†’ keep neutral backups with simplified ACLs  
- **Recovery Practice:** Test restores on alternate systems  

---

## Phase 5: Network & Connectivity Evolution
- **Initial Setup:** USB Wiâ€‘Fi adapter + Cisco switch (200â€¯W draw)  
- **Optimization:** Replaced Cisco switch with domestic router (25â€¯W)  
- **Final Step:** Independent mini PC with pfSense firewall + Cat6a direct link â†’ simpler, segmented, more secure, after experimentation with pfSense in VM first to determine its capability.  

---

## Phase 5.5: Handsâ€‘On Networking & Certification Prep
- **Motivation:** CompTIA virtual labs were unstable â†’ opted for real hardware  
- **Hardware:** Cisco Layer 3 switch (secondâ€‘hand)  
- **Software:**  
  - Cisco Packet Tracer â†’ lightweight, stable, accurate for CCNA  
  - GNS3 â†’ heavier, integrates with real VMs  
- **Learning Path:** Completed *Cisco_CCNA_Lab_Guide_v200â€‘301f*  
- **Outcome:** Packet Tracer sufficient for CCNA prep; real hardware taught quirks (e.g., no power switch, must pull plug)  

---

## Phase 6: Workstation Acquisition & Overprovisioning
- **Specs:**  
  - HP workstation, dual Xeons (24 cores / 48 threads)  
  - 64â€¯GB ECC RAM  
  - Nvidia Quadro M6000 GPU
  - Low price of $600, sounded like a good starting point.
- **Realization:**  
  - Overprovisioned: rarely needed >12 cores for ~5 VMs  
  - Xeon clock speed lower than consumer CPUs  
  - Beneficial for Hashcat, but GPU (â‰ˆ3000 CUDA cores) still dominated
  - Required 2nd GPU if passthrough is required. CUDA work done in Windows and seperate OS on USB stick.
- **Expansion:**  
  - Second workstation for backups/personal use (6â€‘core Xeon @ 3.5â€¯GHz)  
  - Both fitted with ASUS Hyper cards (4â€¯TB initially NVMe, eventually 4TB more in back up machine).  
- **Lesson:** Install Hyper cards fully populated upfront â€” thermal paste adhesion makes later upgrades risky  

---

## Phase 6.5: Power Draw, PSU, and Operational Quirks
- **Power Consumption:**  
  - Dual Xeons ~240â€¯W max, ~130â€¯W idle  
  - Full load with GPU CUDA: 550â€“600â€¯W  
- **PSU Selection:**  
  - Must size for peak load  
  - 850â€¯W 80+ Gold/Platinum ideal for efficiency at 200â€“400â€¯W typical draw  
- **BIOS/Enterprise Quirks:**  
  - Defaults often RAID, not AHCI â†’ OS fails after CMOS reset  
  - Bifurcation must be enabled for ASUS Hyper card  
  - RDP requires GPU if CPU lacks iGPU â†’ hidden dependency  
- **Safeguards:**  
  - Document baseline configs (RAID/AHCI, bifurcation, virtualization flags)  
  - Keep USB recovery media ready  

---

## Phase 7: Device Segmentation & Experimentation
- **Roles:**  
  - Main workstation â†’ virtualization/experimentation  
  - Backup workstation â†’ cold storage, private docs  
  - Laptop â†’ attack/test system  
  - Tablet (rooted) â†’ Wiâ€‘Fi testing (later replaced by laptop + dongle, since required device rooted and developer options enabled at all times)  

---

## ðŸ”„ Alternative Considerations for what I should have done instead
- **NUC (6â€“8 cores):** Efficient, low power, but no PCIe x16 â†’ no GPU; reliant on external/NAS storage  
- **Dedicated NAS:** Smarter for cold storage than NVMe; slower but safer and expandable  
- **mITX Build:** Supports discrete GPU, lower power than Xeon workstation, but higher upfront cost  

---

## ðŸ“Œ Key Lessons Learned
- **Empirical Diagnosis:** Always validate bottlenecks (disk vs CPU vs RAM)  
- **I/O First:** Storage throughput is the primary limiter in virtualization  
- **Incremental Upgrades:** SSD â†’ NVMe â†’ PCIe bifurcation, each step justified by measured gains  
- **Backups vs Redundancy:** RAID â‰  backup; true resilience requires versioned, portable backups  
- **VM Portability:** Test restores on alternate systems to confirm portability, extra drive space and CPU cores allow for full replication in my circumstance, drive space is main requirement for this on NUC or mITX
- **Networking:** Packet Tracer is sufficient for CCNA; GNS3 adds realism; real hardware teaches quirks  
- **Power Economics:** PSU efficiency and idle draw matter as much as peak performance  
- **Enterprise Quirks:** BIOS defaults, bifurcation, GPU dependencies â€” document configs for recovery  
- **Segmentation:** Isolate workloads (personal vs lab) to reduce risk  
- **Efficiency vs Capability:** NUCs, workstations, and mITX each have tradeâ€‘offs in cost, expandability, and power  

